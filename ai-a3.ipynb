{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f929a47",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-23T06:32:48.815844Z",
     "iopub.status.busy": "2024-10-23T06:32:48.815467Z",
     "iopub.status.idle": "2024-10-23T06:32:50.809901Z",
     "shell.execute_reply": "2024-10-23T06:32:50.808610Z"
    },
    "papermill": {
     "duration": 2.002256,
     "end_time": "2024-10-23T06:32:50.812307",
     "exception": false,
     "start_time": "2024-10-23T06:32:48.810051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /kaggle/input/setupfiles/environment.yml /kaggle/working/\n",
    "!cp /kaggle/input/setupfiles/install.sh /kaggle/working/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9019b9d",
   "metadata": {
    "papermill": {
     "duration": 0.0032,
     "end_time": "2024-10-23T06:32:50.819387",
     "exception": false,
     "start_time": "2024-10-23T06:32:50.816187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dependency Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6801161d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T06:32:50.827625Z",
     "iopub.status.busy": "2024-10-23T06:32:50.827257Z",
     "iopub.status.idle": "2024-10-23T06:33:03.394631Z",
     "shell.execute_reply": "2024-10-23T06:33:03.393524Z"
    },
    "papermill": {
     "duration": 12.574234,
     "end_time": "2024-10-23T06:33:03.396965",
     "exception": false,
     "start_time": "2024-10-23T06:32:50.822731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies from environment.yml using pip...\r\n",
      "Environment setup completed successfully!\r\n"
     ]
    }
   ],
   "source": [
    "!bash install.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a227e4a9",
   "metadata": {
    "papermill": {
     "duration": 0.003483,
     "end_time": "2024-10-23T06:33:03.404329",
     "exception": false,
     "start_time": "2024-10-23T06:33:03.400846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "100a0b98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T06:33:03.413107Z",
     "iopub.status.busy": "2024-10-23T06:33:03.412737Z",
     "iopub.status.idle": "2024-10-23T06:33:09.501269Z",
     "shell.execute_reply": "2024-10-23T06:33:09.500259Z"
    },
    "papermill": {
     "duration": 6.09574,
     "end_time": "2024-10-23T06:33:09.503729",
     "exception": false,
     "start_time": "2024-10-23T06:33:03.407989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdb121a",
   "metadata": {
    "papermill": {
     "duration": 0.003413,
     "end_time": "2024-10-23T06:33:09.511014",
     "exception": false,
     "start_time": "2024-10-23T06:33:09.507601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Class Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5651ee6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T06:33:09.520974Z",
     "iopub.status.busy": "2024-10-23T06:33:09.520512Z",
     "iopub.status.idle": "2024-10-23T06:33:09.551017Z",
     "shell.execute_reply": "2024-10-23T06:33:09.550079Z"
    },
    "papermill": {
     "duration": 0.038468,
     "end_time": "2024-10-23T06:33:09.552979",
     "exception": false,
     "start_time": "2024-10-23T06:33:09.514511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class birdClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(birdClassifier, self).__init__()\n",
    "        print(\"Initializing birdClassifier\")\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),  # Conv1: 64 filters\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # Conv2: 64 filters\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Pooling reduces size by half\n",
    "            # Block 2\n",
    "            nn.Conv2d(\n",
    "                64, 128, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv3: 128 filters\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                128, 128, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv4: 128 filters\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Block 3\n",
    "            nn.Conv2d(\n",
    "                128, 256, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv5: 256 filters\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                256, 256, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv6: 256 filters\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                256, 256, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv7: 256 filters\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Block 4\n",
    "            nn.Conv2d(\n",
    "                256, 512, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv8: 512 filters\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                512, 512, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv9: 512 filters\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                512, 512, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv10: 512 filters\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Block 5\n",
    "            nn.Conv2d(\n",
    "                512, 1024, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv11: 1024 filters\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                1024, 1024, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv12: 1024 filters\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * 7 * 7, 4096),  # Fully connected layer 1\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Dropout for regularization\n",
    "            nn.Linear(4096, 4096),  # Fully connected layer 2\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 10),  # Output layer (10 bird classes)\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction=\"mean\"):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=60,\n",
    "    patience=10,\n",
    "):\n",
    "    print(\"Starting model training\")\n",
    "    model.train()\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_without_improvement = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 100 == 99:  # Print every 100 mini-batches\n",
    "                print(\n",
    "                    f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 100:.6f}\"\n",
    "                )\n",
    "                print(f\"Sample outputs: {outputs[0][:5]}\")\n",
    "                print(f\"Sample labels: {labels[0]}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch + 1} Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4df99d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T06:33:09.561697Z",
     "iopub.status.busy": "2024-10-23T06:33:09.561374Z",
     "iopub.status.idle": "2024-10-23T06:34:18.290962Z",
     "shell.execute_reply": "2024-10-23T06:34:18.289405Z"
    },
    "papermill": {
     "duration": 68.736884,
     "end_time": "2024-10-23T06:34:18.293576",
     "exception": false,
     "start_time": "2024-10-23T06:33:09.556692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started\n",
      "Data path: /kaggle/input/identify-the-birds/Birds/train\n",
      "Train status: train\n",
      "Model path: --HistoryManager.hist_file=:memory:\n",
      "Entering training mode\n",
      "Label distribution: {3: 542, 6: 761, 4: 770, 7: 606, 9: 527, 1: 610, 5: 770, 0: 693, 8: 631, 2: 464}\n",
      "Input data range: [0.00, 0.97]\n"
     ]
    }
   ],
   "source": [
    "print(\"Script started\")\n",
    "dataPath = \"/kaggle/input/identify-the-birds/Birds/train\"\n",
    "trainStatus = \"train\"\n",
    "modelPath = sys.argv[3] if len(sys.argv) > 3 else \"model.pth\"\n",
    "\n",
    "print(f\"Data path: {dataPath}\")\n",
    "print(f\"Train status: {trainStatus}\")\n",
    "print(f\"Model path: {modelPath}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Entering training mode\")\n",
    "\n",
    "# Set up data transformations\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "full_dataset = datasets.ImageFolder(root=dataPath, transform=transform)\n",
    "\n",
    "# Create train-test split\n",
    "train_indices, test_indices = train_test_split(\n",
    "    range(len(full_dataset)),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=full_dataset.targets,\n",
    ")\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "label_counts = {}\n",
    "for _, labels in train_loader:\n",
    "    for label in labels:\n",
    "        label_counts[label.item()] = label_counts.get(label.item(), 0) + 1\n",
    "print(\"Label distribution:\", label_counts)\n",
    "\n",
    "\n",
    "# Check input data range\n",
    "for inputs, labels in train_loader:\n",
    "    print(f\"Input data range: [{inputs.min().item():.2f}, {inputs.max().item():.2f}]\")\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3d939a",
   "metadata": {
    "papermill": {
     "duration": 0.004357,
     "end_time": "2024-10-23T06:34:18.302936",
     "exception": false,
     "start_time": "2024-10-23T06:34:18.298579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Run your .py file on CLI using the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "637779b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T06:34:18.313456Z",
     "iopub.status.busy": "2024-10-23T06:34:18.312569Z",
     "iopub.status.idle": "2024-10-23T06:52:24.677384Z",
     "shell.execute_reply": "2024-10-23T06:52:24.676182Z"
    },
    "papermill": {
     "duration": 1086.385941,
     "end_time": "2024-10-23T06:52:24.693285",
     "exception": false,
     "start_time": "2024-10-23T06:34:18.307344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing birdClassifier\n",
      "Model created and moved to device\n",
      "Starting model training\n",
      "[Epoch 1, Batch 100] Loss: 1.928953\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "[Epoch 1, Batch 200] Loss: 1.995173\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 5\n",
      "[Epoch 1, Batch 300] Loss: 1.991183\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 9\n",
      "[Epoch 1, Batch 400] Loss: 2.003154\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "[Epoch 1, Batch 500] Loss: 2.003154\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 5\n",
      "[Epoch 1, Batch 600] Loss: 1.971231\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 9\n",
      "[Epoch 1, Batch 700] Loss: 1.971231\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "[Epoch 1, Batch 800] Loss: 1.995173\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 8\n",
      "[Epoch 1, Batch 900] Loss: 1.979211\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 9\n",
      "[Epoch 1, Batch 1000] Loss: 1.983202\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "[Epoch 1, Batch 1100] Loss: 1.987192\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 1\n",
      "[Epoch 1, Batch 1200] Loss: 1.919355\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 8\n",
      "[Epoch 1, Batch 1300] Loss: 1.959259\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 1\n",
      "[Epoch 1, Batch 1400] Loss: 1.911374\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 1\n",
      "[Epoch 1, Batch 1500] Loss: 1.975221\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 1\n",
      "[Epoch 1, Batch 1600] Loss: 1.959122\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 2\n",
      "[Epoch 1, Batch 1700] Loss: 1.955269\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "[Epoch 1, Batch 1800] Loss: 1.983202\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "[Epoch 1, Batch 1900] Loss: 1.935317\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 1, Batch 2000] Loss: 1.963250\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 9\n",
      "[Epoch 1, Batch 2100] Loss: 1.943298\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 9\n",
      "Epoch 1 Validation Loss: 1.950012\n",
      "[Epoch 2, Batch 100] Loss: 1.963250\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 5\n",
      "[Epoch 2, Batch 200] Loss: 1.935317\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 5\n",
      "[Epoch 2, Batch 300] Loss: 1.959259\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 1\n",
      "[Epoch 2, Batch 400] Loss: 1.979211\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 2, Batch 500] Loss: 1.979211\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 1\n",
      "[Epoch 2, Batch 600] Loss: 1.943298\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 5\n",
      "[Epoch 2, Batch 700] Loss: 1.939307\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "[Epoch 2, Batch 800] Loss: 1.967240\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "[Epoch 2, Batch 900] Loss: 1.935317\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 9\n",
      "[Epoch 2, Batch 1000] Loss: 1.967240\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 5\n",
      "[Epoch 2, Batch 1100] Loss: 1.943298\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "[Epoch 2, Batch 1200] Loss: 1.963250\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 2, Batch 1300] Loss: 1.935317\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 2, Batch 1400] Loss: 1.955269\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 8\n",
      "[Epoch 2, Batch 1500] Loss: 1.955269\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 2\n",
      "[Epoch 2, Batch 1600] Loss: 1.979211\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "[Epoch 2, Batch 1700] Loss: 1.935317\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 2\n",
      "[Epoch 2, Batch 1800] Loss: 1.999164\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 7\n",
      "[Epoch 2, Batch 1900] Loss: 1.955269\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 8\n",
      "[Epoch 2, Batch 2000] Loss: 1.983202\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "[Epoch 2, Batch 2100] Loss: 1.987192\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 1\n",
      "Epoch 2 Validation Loss: 1.950012\n",
      "[Epoch 3, Batch 100] Loss: 1.951278\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "[Epoch 3, Batch 200] Loss: 1.959259\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "[Epoch 3, Batch 300] Loss: 1.959259\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 3, Batch 400] Loss: 1.999164\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 3, Batch 500] Loss: 1.947288\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 3, Batch 600] Loss: 1.911374\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 9\n",
      "[Epoch 3, Batch 700] Loss: 1.963250\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 2\n",
      "[Epoch 3, Batch 800] Loss: 1.951278\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "[Epoch 3, Batch 900] Loss: 1.975221\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 9\n",
      "[Epoch 3, Batch 1000] Loss: 1.935317\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 2\n",
      "[Epoch 3, Batch 1100] Loss: 1.943298\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "[Epoch 3, Batch 1200] Loss: 1.959259\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 7\n",
      "[Epoch 3, Batch 1300] Loss: 1.983202\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "[Epoch 3, Batch 1400] Loss: 1.979211\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 8\n",
      "[Epoch 3, Batch 1500] Loss: 1.951278\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 5\n",
      "[Epoch 3, Batch 1600] Loss: 1.959259\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "[Epoch 3, Batch 1700] Loss: 1.983202\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 1\n",
      "[Epoch 3, Batch 1800] Loss: 1.963250\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "[Epoch 3, Batch 1900] Loss: 1.975221\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 5\n",
      "[Epoch 3, Batch 2000] Loss: 1.951278\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "[Epoch 3, Batch 2100] Loss: 1.955269\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 1\n",
      "Epoch 3 Validation Loss: 1.950012\n",
      "[Epoch 4, Batch 100] Loss: 1.943298\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "[Epoch 4, Batch 200] Loss: 2.003154\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 3\n",
      "[Epoch 4, Batch 300] Loss: 1.955269\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 9\n",
      "[Epoch 4, Batch 400] Loss: 1.943298\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 7\n",
      "[Epoch 4, Batch 500] Loss: 1.939307\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 2\n",
      "[Epoch 4, Batch 600] Loss: 1.959259\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 7\n",
      "[Epoch 4, Batch 700] Loss: 1.971231\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 4, Batch 800] Loss: 1.931326\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 1\n",
      "[Epoch 4, Batch 900] Loss: 1.963250\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "[Epoch 4, Batch 1000] Loss: 1.947288\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "[Epoch 4, Batch 1100] Loss: 1.971231\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 4, Batch 1200] Loss: 1.987192\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 8\n",
      "[Epoch 4, Batch 1300] Loss: 1.943298\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 9\n",
      "[Epoch 4, Batch 1400] Loss: 1.955269\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "[Epoch 4, Batch 1500] Loss: 1.963250\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 5\n",
      "[Epoch 4, Batch 1600] Loss: 1.943298\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 9\n",
      "[Epoch 4, Batch 1700] Loss: 1.975221\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 7\n",
      "[Epoch 4, Batch 1800] Loss: 1.975221\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 3\n",
      "[Epoch 4, Batch 1900] Loss: 1.923345\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 5\n",
      "[Epoch 4, Batch 2000] Loss: 1.983202\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 4, Batch 2100] Loss: 1.983202\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "Epoch 4 Validation Loss: 1.950012\n",
      "[Epoch 5, Batch 100] Loss: 1.967240\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 5\n",
      "[Epoch 5, Batch 200] Loss: 1.959259\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 5, Batch 300] Loss: 1.959259\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 5, Batch 400] Loss: 1.991183\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 5\n",
      "[Epoch 5, Batch 500] Loss: 1.979211\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 3\n",
      "[Epoch 5, Batch 600] Loss: 1.935317\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "[Epoch 5, Batch 700] Loss: 1.951278\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 3\n",
      "[Epoch 5, Batch 800] Loss: 1.967240\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "[Epoch 5, Batch 900] Loss: 1.975221\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 5\n",
      "[Epoch 5, Batch 1000] Loss: 1.963250\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 8\n",
      "[Epoch 5, Batch 1100] Loss: 1.943298\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "[Epoch 5, Batch 1200] Loss: 1.915364\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "[Epoch 5, Batch 1300] Loss: 1.975221\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 5, Batch 1400] Loss: 1.967240\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 3\n",
      "[Epoch 5, Batch 1500] Loss: 1.927336\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 5, Batch 1600] Loss: 1.955269\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 7\n",
      "[Epoch 5, Batch 1700] Loss: 1.983202\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "[Epoch 5, Batch 1800] Loss: 1.931326\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 3\n",
      "[Epoch 5, Batch 1900] Loss: 1.959259\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 5, Batch 2000] Loss: 1.983202\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "[Epoch 5, Batch 2100] Loss: 1.963250\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 5\n",
      "Epoch 5 Validation Loss: 1.950012\n",
      "[Epoch 6, Batch 100] Loss: 1.911374\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 7\n",
      "[Epoch 6, Batch 200] Loss: 1.967240\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "[Epoch 6, Batch 300] Loss: 1.943298\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 6, Batch 400] Loss: 1.951278\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 2\n",
      "[Epoch 6, Batch 500] Loss: 1.955269\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 1\n",
      "[Epoch 6, Batch 600] Loss: 1.971231\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 6, Batch 700] Loss: 1.967240\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 8\n",
      "[Epoch 6, Batch 800] Loss: 1.963250\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "[Epoch 6, Batch 900] Loss: 1.971231\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 6, Batch 1000] Loss: 1.975221\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 5\n",
      "[Epoch 6, Batch 1100] Loss: 1.967240\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 1\n",
      "[Epoch 6, Batch 1200] Loss: 1.963250\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 6, Batch 1300] Loss: 1.955269\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 9\n",
      "[Epoch 6, Batch 1400] Loss: 1.975221\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "[Epoch 6, Batch 1500] Loss: 1.963250\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "[Epoch 6, Batch 1600] Loss: 1.967240\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 5\n",
      "[Epoch 6, Batch 1700] Loss: 1.947288\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 2\n",
      "[Epoch 6, Batch 1800] Loss: 1.987192\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 9\n",
      "[Epoch 6, Batch 1900] Loss: 1.959259\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 7\n",
      "[Epoch 6, Batch 2000] Loss: 1.919355\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 9\n",
      "[Epoch 6, Batch 2100] Loss: 1.967240\n",
      "Sample outputs: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "Epoch 6 Validation Loss: 1.950012\n",
      "Early stopping triggered after 6 epochs\n",
      "Finished Training\n",
      "Model saved to --HistoryManager.hist_file=:memory:\n",
      "Accuracy on the test set: 8.43%\n"
     ]
    }
   ],
   "source": [
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model instance\n",
    "model = birdClassifier().to(device)\n",
    "print(\"Model created and moved to device\")\n",
    "# Set up loss function and optimizer\n",
    "criterion = FocalLoss(alpha=1, gamma=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=60,\n",
    "    patience=5,\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), modelPath)\n",
    "print(f\"Model saved to {modelPath}\")\n",
    "# hello world\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy on the test set: {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5837237,
     "sourceId": 9575326,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5928001,
     "sourceId": 9695725,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1181.168543,
   "end_time": "2024-10-23T06:52:27.079290",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-23T06:32:45.910747",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
