{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd7e348",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-23T07:16:46.531074Z",
     "iopub.status.busy": "2024-10-23T07:16:46.529841Z",
     "iopub.status.idle": "2024-10-23T07:16:48.521427Z",
     "shell.execute_reply": "2024-10-23T07:16:48.520380Z"
    },
    "papermill": {
     "duration": 2.000117,
     "end_time": "2024-10-23T07:16:48.523752",
     "exception": false,
     "start_time": "2024-10-23T07:16:46.523635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /kaggle/input/setupfiles/environment.yml /kaggle/working/\n",
    "!cp /kaggle/input/setupfiles/install.sh /kaggle/working/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d8919",
   "metadata": {
    "papermill": {
     "duration": 0.003463,
     "end_time": "2024-10-23T07:16:48.531082",
     "exception": false,
     "start_time": "2024-10-23T07:16:48.527619",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dependency Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ab4254",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T07:16:48.539382Z",
     "iopub.status.busy": "2024-10-23T07:16:48.538993Z",
     "iopub.status.idle": "2024-10-23T07:17:00.880136Z",
     "shell.execute_reply": "2024-10-23T07:17:00.879093Z"
    },
    "papermill": {
     "duration": 12.347931,
     "end_time": "2024-10-23T07:17:00.882494",
     "exception": false,
     "start_time": "2024-10-23T07:16:48.534563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies from environment.yml using pip...\r\n",
      "Environment setup completed successfully!\r\n"
     ]
    }
   ],
   "source": [
    "!bash install.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84d7c6c",
   "metadata": {
    "papermill": {
     "duration": 0.003628,
     "end_time": "2024-10-23T07:17:00.890154",
     "exception": false,
     "start_time": "2024-10-23T07:17:00.886526",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97e95899",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T07:17:00.899049Z",
     "iopub.status.busy": "2024-10-23T07:17:00.898421Z",
     "iopub.status.idle": "2024-10-23T07:17:06.915627Z",
     "shell.execute_reply": "2024-10-23T07:17:06.914789Z"
    },
    "papermill": {
     "duration": 6.024258,
     "end_time": "2024-10-23T07:17:06.917968",
     "exception": false,
     "start_time": "2024-10-23T07:17:00.893710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc4867f",
   "metadata": {
    "papermill": {
     "duration": 0.003456,
     "end_time": "2024-10-23T07:17:06.925323",
     "exception": false,
     "start_time": "2024-10-23T07:17:06.921867",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Class Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "307c64a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T07:17:06.935262Z",
     "iopub.status.busy": "2024-10-23T07:17:06.934792Z",
     "iopub.status.idle": "2024-10-23T07:17:06.965405Z",
     "shell.execute_reply": "2024-10-23T07:17:06.964491Z"
    },
    "papermill": {
     "duration": 0.038405,
     "end_time": "2024-10-23T07:17:06.967236",
     "exception": false,
     "start_time": "2024-10-23T07:17:06.928831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class birdClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(birdClassifier, self).__init__()\n",
    "        print(\"Initializing birdClassifier\")\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),  # Conv1: 64 filters\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # Conv2: 64 filters\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Pooling reduces size by half\n",
    "            # Block 2\n",
    "            nn.Conv2d(\n",
    "                64, 128, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv3: 128 filters\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                128, 128, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv4: 128 filters\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Block 3\n",
    "            nn.Conv2d(\n",
    "                128, 256, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv5: 256 filters\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                256, 256, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv6: 256 filters\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                256, 256, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv7: 256 filters\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Block 4\n",
    "            nn.Conv2d(\n",
    "                256, 512, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv8: 512 filters\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                512, 512, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv9: 512 filters\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                512, 512, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv10: 512 filters\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Block 5\n",
    "            nn.Conv2d(\n",
    "                512, 1024, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv11: 1024 filters\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                1024, 1024, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Conv12: 1024 filters\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * 7 * 7, 4096),  # Fully connected layer 1\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Dropout for regularization\n",
    "            nn.Linear(4096, 4096),  # Fully connected layer 2\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 10))  # Output layer (10 bird classes))\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction=\"mean\"):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=60,\n",
    "    patience=10,\n",
    "):\n",
    "    print(\"Starting model training\")\n",
    "    model.train()\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_without_improvement = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 100 == 99:  # Print every 100 mini-batches\n",
    "                print(\n",
    "                    f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 100:.6f}\"\n",
    "                )\n",
    "                print(f\"Sample outputs: {outputs[0][:5]}\")\n",
    "                print(f\"Sample labels: {labels[0]}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch + 1} Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b7b7ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T07:17:06.975552Z",
     "iopub.status.busy": "2024-10-23T07:17:06.975226Z",
     "iopub.status.idle": "2024-10-23T07:18:12.548014Z",
     "shell.execute_reply": "2024-10-23T07:18:12.547002Z"
    },
    "papermill": {
     "duration": 65.579651,
     "end_time": "2024-10-23T07:18:12.550397",
     "exception": false,
     "start_time": "2024-10-23T07:17:06.970746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started\n",
      "Data path: /kaggle/input/identify-the-birds/Birds/train\n",
      "Train status: train\n",
      "Model path: --HistoryManager.hist_file=:memory:\n",
      "Entering training mode\n",
      "Label distribution: {3: 533, 0: 688, 2: 461, 7: 622, 6: 752, 5: 755, 4: 777, 8: 633, 1: 604, 9: 549}\n",
      "Input data range: [0.00, 1.00]\n"
     ]
    }
   ],
   "source": [
    "print(\"Script started\")\n",
    "dataPath = \"/kaggle/input/identify-the-birds/Birds/train\"\n",
    "trainStatus = \"train\"\n",
    "modelPath = sys.argv[3] if len(sys.argv) > 3 else \"model.pth\"\n",
    "\n",
    "print(f\"Data path: {dataPath}\")\n",
    "print(f\"Train status: {trainStatus}\")\n",
    "print(f\"Model path: {modelPath}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Entering training mode\")\n",
    "\n",
    "# Set up data transformations\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "full_dataset = datasets.ImageFolder(root=dataPath, transform=transform)\n",
    "\n",
    "# Create train-test split\n",
    "train_indices, test_indices = train_test_split(\n",
    "    range(len(full_dataset)),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=full_dataset.targets,\n",
    ")\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "label_counts = {}\n",
    "for _, labels in train_loader:\n",
    "    for label in labels:\n",
    "        label_counts[label.item()] = label_counts.get(label.item(), 0) + 1\n",
    "print(\"Label distribution:\", label_counts)\n",
    "\n",
    "\n",
    "# Check input data range\n",
    "for inputs, labels in train_loader:\n",
    "    print(f\"Input data range: [{inputs.min().item():.2f}, {inputs.max().item():.2f}]\")\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca4644c",
   "metadata": {
    "papermill": {
     "duration": 0.003617,
     "end_time": "2024-10-23T07:18:12.558140",
     "exception": false,
     "start_time": "2024-10-23T07:18:12.554523",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Run your .py file on CLI using the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17747f26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T07:18:12.567487Z",
     "iopub.status.busy": "2024-10-23T07:18:12.567136Z",
     "iopub.status.idle": "2024-10-23T07:26:20.798455Z",
     "shell.execute_reply": "2024-10-23T07:26:20.797334Z"
    },
    "papermill": {
     "duration": 488.242884,
     "end_time": "2024-10-23T07:26:20.804820",
     "exception": false,
     "start_time": "2024-10-23T07:18:12.561936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing birdClassifier\n",
      "Model created and moved to device\n",
      "Starting model training\n",
      "[Epoch 1, Batch 100] Loss: 23.326809\n",
      "Sample outputs: tensor([-0.0591,  0.1838, -0.5546, -0.1036,  0.0342], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "[Epoch 1, Batch 200] Loss: 9.322057\n",
      "Sample outputs: tensor([ 0.1907,  0.1446,  0.0989, -0.3468,  0.3115], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "Epoch 1 Validation Loss: 1.877586\n",
      "[Epoch 2, Batch 100] Loss: 4.116988\n",
      "Sample outputs: tensor([-0.2468, -0.6380, -0.0272,  0.6722,  0.2031], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Sample labels: 8\n",
      "[Epoch 2, Batch 200] Loss: 1.944708\n",
      "Sample outputs: tensor([-0.0693,  0.0544,  0.0581,  0.2617,  0.7995], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Sample labels: 2\n",
      "Epoch 2 Validation Loss: 3.016351\n",
      "[Epoch 3, Batch 100] Loss: 1.894989\n",
      "Sample outputs: tensor([ 0.2158,  0.0541, -0.1996, -0.1187,  0.5147], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Sample labels: 9\n",
      "[Epoch 3, Batch 200] Loss: 1.869206\n",
      "Sample outputs: tensor([ 0.1160,  0.1852, -0.3707,  0.1964,  0.3155], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Sample labels: 1\n",
      "Epoch 3 Validation Loss: 3.349553\n",
      "[Epoch 4, Batch 100] Loss: 1.876187\n",
      "Sample outputs: tensor([ 0.2987, -0.1907, -0.1766, -0.0104,  0.5420], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Sample labels: 1\n",
      "[Epoch 4, Batch 200] Loss: 1.861541\n",
      "Sample outputs: tensor([ 0.1513,  0.1566, -0.0190, -0.2706,  0.4997], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Sample labels: 6\n",
      "Epoch 4 Validation Loss: 3.023990\n",
      "[Epoch 5, Batch 100] Loss: 1.873547\n",
      "Sample outputs: tensor([ 0.0034, -0.0774,  0.0068,  0.1624,  0.3317], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Sample labels: 0\n",
      "[Epoch 5, Batch 200] Loss: 1.859393\n",
      "Sample outputs: tensor([-0.0340, -0.0017, -0.1824,  0.2128,  0.3043], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Sample labels: 1\n",
      "Epoch 5 Validation Loss: 4.272369\n",
      "[Epoch 6, Batch 100] Loss: 1.868794\n",
      "Sample outputs: tensor([ 0.2082,  0.3357, -0.2954, -0.1857,  0.4089], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Sample labels: 4\n",
      "[Epoch 6, Batch 200] Loss: 1.863924\n",
      "Sample outputs: tensor([ 0.4304,  0.0537, -0.2466,  0.0214,  0.0647], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Sample labels: 3\n",
      "Epoch 6 Validation Loss: 2.268462\n",
      "Early stopping triggered after 6 epochs\n",
      "Finished Training\n",
      "Model saved to --HistoryManager.hist_file=:memory:\n",
      "Accuracy on the test set: 10.64%\n"
     ]
    }
   ],
   "source": [
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model instance\n",
    "model = birdClassifier().to(device)\n",
    "print(\"Model created and moved to device\")\n",
    "# Set up loss function and optimizer\n",
    "criterion = FocalLoss(alpha=1, gamma=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=60,\n",
    "    patience=5,\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), modelPath)\n",
    "print(f\"Model saved to {modelPath}\")\n",
    "# hello world\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy on the test set: {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5837237,
     "sourceId": 9575326,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5928001,
     "sourceId": 9695725,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 578.383718,
   "end_time": "2024-10-23T07:26:22.232529",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-23T07:16:43.848811",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
